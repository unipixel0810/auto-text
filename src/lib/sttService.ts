/**
 * STT (Speech-to-Text) ì„œë¹„ìŠ¤
 * OpenAI Whisper APIë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹
 * ëŒ€ìš©ëŸ‰ íŒŒì¼ ì§€ì› (ìµœëŒ€ 2GB)
 */

import type { WordTimestamp, STTResult } from './subtitleSplitter';

// ============================================
// íƒ€ì… ì •ì˜
// ============================================

export interface WhisperSegment {
  id: number;
  seek: number;
  start: number;
  end: number;
  text: string;
  tokens: number[];
  temperature: number;
  avg_logprob: number;
  compression_ratio: number;
  no_speech_prob: number;
}

export interface WhisperWord {
  word: string;
  start: number;
  end: number;
}

export interface WhisperResponse {
  task: string;
  language: string;
  duration: number;
  text: string;
  segments?: WhisperSegment[];
  words?: WhisperWord[];
}

export interface STTOptions {
  /** OpenAI API í‚¤ (ì„œë²„ API Route ì‚¬ìš© ì‹œ ë¶ˆí•„ìš”) */
  apiKey?: string;
  /** ì–¸ì–´ ì½”ë“œ (ê¸°ë³¸ê°’: 'ko') */
  language?: string;
  /** ì‘ë‹µ í˜•ì‹ */
  responseFormat?: 'json' | 'verbose_json';
  /** íƒ€ì„ìŠ¤íƒ¬í”„ ì„¸ë¶„í™” */
  timestampGranularities?: ('word' | 'segment')[];
}

// ============================================
// ìƒìˆ˜
// ============================================

/** Vercel ì„œë²„ë¦¬ìŠ¤ í•¨ìˆ˜ ì œí•œ (4MBë¡œ ì•ˆì „í•˜ê²Œ) */
const VERCEL_MAX_SIZE = 4 * 1024 * 1024;

/** Whisper API ìµœëŒ€ íŒŒì¼ í¬ê¸° (25MB) */
const WHISPER_MAX_SIZE = 25 * 1024 * 1024;

/** ì˜¤ë””ì˜¤ ì²­í¬ ê¸¸ì´ (2ë¶„ - ì‘ê²Œ ìœ ì§€) */
const CHUNK_DURATION_SECONDS = 120;

/** ì§€ì›í•˜ëŠ” ìµœëŒ€ íŒŒì¼ í¬ê¸° (400GB) */
const MAX_FILE_SIZE = 400 * 1024 * 1024 * 1024;

// ============================================
// ì˜¤ë””ì˜¤ ì¶”ì¶œ (Web Audio API)
// ============================================

/**
 * ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ì¶”ì¶œ ë° ì••ì¶•
 */
export async function extractAudioFromVideo(
  videoFile: File,
  onProgress?: (status: string) => void
): Promise<Blob> {
  return new Promise((resolve, reject) => {
    onProgress?.('ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤€ë¹„ ì¤‘...');
    
    const video = document.createElement('video');
    video.src = URL.createObjectURL(videoFile);
    video.muted = false;
    
    video.onloadedmetadata = async () => {
      try {
        const duration = video.duration;
        onProgress?.(`ì˜ìƒ ê¸¸ì´: ${Math.floor(duration / 60)}ë¶„ ${Math.floor(duration % 60)}ì´ˆ`);
        
        // AudioContext ìƒì„±
        const audioContext = new AudioContext({ sampleRate: 16000 });
        
        // ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        onProgress?.('ì˜¤ë””ì˜¤ ë””ì½”ë”© ì¤‘...');
        
        const response = await fetch(video.src);
        const arrayBuffer = await response.arrayBuffer();
        
        let audioBuffer: AudioBuffer;
        try {
          audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
        } catch {
          // ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ ì›ë³¸ íŒŒì¼ ë°˜í™˜ (Whisperê°€ ì§ì ‘ ì²˜ë¦¬)
          URL.revokeObjectURL(video.src);
          onProgress?.('ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ (ì›ë³¸ ì‚¬ìš©)');
          resolve(videoFile);
          return;
        }
        
        onProgress?.('ì˜¤ë””ì˜¤ ì¸ì½”ë”© ì¤‘...');
        
        // WAV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        const wavBlob = audioBufferToWav(audioBuffer);
        
        URL.revokeObjectURL(video.src);
        onProgress?.('ì˜¤ë””ì˜¤ ì¶”ì¶œ ì™„ë£Œ');
        resolve(wavBlob);
        
      } catch (error) {
        URL.revokeObjectURL(video.src);
        // ì‹¤íŒ¨ ì‹œ ì›ë³¸ íŒŒì¼ ì‚¬ìš©
        resolve(videoFile);
      }
    };
    
    video.onerror = () => {
      URL.revokeObjectURL(video.src);
      reject(new Error('ë¹„ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨'));
    };
  });
}

/**
 * AudioBufferë¥¼ WAV Blobìœ¼ë¡œ ë³€í™˜
 */
function audioBufferToWav(buffer: AudioBuffer): Blob {
  const numChannels = 1; // ëª¨ë…¸ë¡œ ë³€í™˜
  const sampleRate = buffer.sampleRate;
  const format = 1; // PCM
  const bitDepth = 16;
  
  // ëª¨ë…¸ë¡œ ë‹¤ìš´ë¯¹ìŠ¤
  const channelData = buffer.getChannelData(0);
  const samples = new Int16Array(channelData.length);
  
  for (let i = 0; i < channelData.length; i++) {
    const s = Math.max(-1, Math.min(1, channelData[i]));
    samples[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
  }
  
  const dataLength = samples.length * 2;
  const bufferLength = 44 + dataLength;
  const arrayBuffer = new ArrayBuffer(bufferLength);
  const view = new DataView(arrayBuffer);
  
  // WAV í—¤ë” ì‘ì„±
  writeString(view, 0, 'RIFF');
  view.setUint32(4, bufferLength - 8, true);
  writeString(view, 8, 'WAVE');
  writeString(view, 12, 'fmt ');
  view.setUint32(16, 16, true);
  view.setUint16(20, format, true);
  view.setUint16(22, numChannels, true);
  view.setUint32(24, sampleRate, true);
  view.setUint32(28, sampleRate * numChannels * bitDepth / 8, true);
  view.setUint16(32, numChannels * bitDepth / 8, true);
  view.setUint16(34, bitDepth, true);
  writeString(view, 36, 'data');
  view.setUint32(40, dataLength, true);
  
  // ì˜¤ë””ì˜¤ ë°ì´í„° ì‘ì„±
  const offset = 44;
  for (let i = 0; i < samples.length; i++) {
    view.setInt16(offset + i * 2, samples[i], true);
  }
  
  return new Blob([arrayBuffer], { type: 'audio/wav' });
}

function writeString(view: DataView, offset: number, string: string) {
  for (let i = 0; i < string.length; i++) {
    view.setUint8(offset + i, string.charCodeAt(i));
  }
}

// ============================================
// ì˜¤ë””ì˜¤ ì²­í¬ ë¶„í• 
// ============================================

/**
 * ì˜¤ë””ì˜¤ Blobì„ ì²­í¬ë¡œ ë¶„í•  (ì‹œê°„ ê¸°ë°˜)
 */
export async function splitAudioIntoChunks(
  audioBlob: Blob,
  chunkDurationSeconds: number = CHUNK_DURATION_SECONDS,
  onProgress?: (status: string) => void
): Promise<{ blob: Blob; startTime: number }[]> {
  // íŒŒì¼ì´ 4MB ì´í•˜ë©´ ë¶„í•  ë¶ˆí•„ìš” (Vercel ì œí•œ)
  if (audioBlob.size <= VERCEL_MAX_SIZE) {
    return [{ blob: audioBlob, startTime: 0 }];
  }
  
  onProgress?.('ëŒ€ìš©ëŸ‰ íŒŒì¼ ë¶„í•  ì¤‘...');
  
  // AudioContextë¡œ ì˜¤ë””ì˜¤ ë¡œë“œ
  const audioContext = new AudioContext();
  const arrayBuffer = await audioBlob.arrayBuffer();
  
  let audioBuffer: AudioBuffer;
  try {
    audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
  } catch {
    // ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ í¬ê¸° ê¸°ë°˜ ë¶„í• 
    return splitBySize(audioBlob, WHISPER_MAX_SIZE);
  }
  
  const duration = audioBuffer.duration;
  const chunks: { blob: Blob; startTime: number }[] = [];
  const numChunks = Math.ceil(duration / chunkDurationSeconds);
  
  for (let i = 0; i < numChunks; i++) {
    const startTime = i * chunkDurationSeconds;
    const endTime = Math.min((i + 1) * chunkDurationSeconds, duration);
    const chunkDuration = endTime - startTime;
    
    onProgress?.(`ì²­í¬ ${i + 1}/${numChunks} ìƒì„± ì¤‘...`);
    
    // ì²­í¬ AudioBuffer ìƒì„±
    const startSample = Math.floor(startTime * audioBuffer.sampleRate);
    const endSample = Math.floor(endTime * audioBuffer.sampleRate);
    const chunkLength = endSample - startSample;
    
    const chunkBuffer = audioContext.createBuffer(
      1, // ëª¨ë…¸
      chunkLength,
      audioBuffer.sampleRate
    );
    
    const sourceData = audioBuffer.getChannelData(0);
    const destData = chunkBuffer.getChannelData(0);
    
    for (let j = 0; j < chunkLength; j++) {
      destData[j] = sourceData[startSample + j] || 0;
    }
    
    const wavBlob = audioBufferToWav(chunkBuffer);
    chunks.push({ blob: wavBlob, startTime });
  }
  
  await audioContext.close();
  return chunks;
}

/**
 * í¬ê¸° ê¸°ë°˜ ë¶„í•  (ì˜¤ë””ì˜¤ ë””ì½”ë”© ì‹¤íŒ¨ ì‹œ í´ë°±)
 */
function splitBySize(blob: Blob, maxSize: number): { blob: Blob; startTime: number }[] {
  const chunks: { blob: Blob; startTime: number }[] = [];
  let offset = 0;
  let chunkIndex = 0;
  
  while (offset < blob.size) {
    const chunkSize = Math.min(maxSize, blob.size - offset);
    const chunk = blob.slice(offset, offset + chunkSize);
    chunks.push({ blob: chunk, startTime: chunkIndex * 600 }); // ì¶”ì • ì‹œê°„
    offset += chunkSize;
    chunkIndex++;
  }
  
  return chunks;
}

// ============================================
// Whisper API í˜¸ì¶œ
// ============================================

/**
 * OpenAI Whisper APIë¡œ ìŒì„± ì¸ì‹ ìˆ˜í–‰ (ì„œë²„ API Route ì‚¬ìš©)
 */
export async function transcribeWithWhisper(
  audioFile: File | Blob,
  options: STTOptions
): Promise<WhisperResponse> {
  const formData = new FormData();
  
  // íŒŒì¼ ì¶”ê°€
  if (audioFile instanceof File) {
    formData.append('file', audioFile);
  } else {
    formData.append('file', audioFile, 'audio.wav');
  }

  // ì„œë²„ API Route í˜¸ì¶œ (API í‚¤ëŠ” ì„œë²„ì—ì„œ ê´€ë¦¬)
  const response = await fetch('/api/stt', {
    method: 'POST',
    body: formData,
  });

  if (!response.ok) {
    const error = await response.json().catch(() => ({ error: response.statusText }));
    throw new Error(`ìŒì„± ì¸ì‹ ì˜¤ë¥˜: ${error.error || response.statusText}`);
  }

  return response.json();
}

// ============================================
// ê²°ê³¼ ë³€í™˜ ë° ë³‘í•©
// ============================================

/**
 * Whisper ì‘ë‹µì„ STTResult í˜•ì‹ìœ¼ë¡œ ë³€í™˜
 */
export function convertWhisperToSTTResult(
  whisperResponse: WhisperResponse,
  timeOffset: number = 0
): STTResult {
  const words: WordTimestamp[] = [];

  // ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆëŠ” ê²½ìš°
  if (whisperResponse.words && whisperResponse.words.length > 0) {
    for (const word of whisperResponse.words) {
      words.push({
        word: word.word.trim(),
        startTime: word.start + timeOffset,
        endTime: word.end + timeOffset,
        confidence: 1,
      });
    }
  } 
  // ì„¸ê·¸ë¨¼íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
  else if (whisperResponse.segments && whisperResponse.segments.length > 0) {
    for (const segment of whisperResponse.segments) {
      const segmentWords = segment.text.trim().split(/\s+/).filter(w => w.length > 0);
      const segmentDuration = segment.end - segment.start;
      const wordDuration = segmentDuration / segmentWords.length;
      
      segmentWords.forEach((word, index) => {
        words.push({
          word,
          startTime: segment.start + timeOffset + (index * wordDuration),
          endTime: segment.start + timeOffset + ((index + 1) * wordDuration),
          confidence: 1 - segment.no_speech_prob,
        });
      });
    }
  }
  // ì „ì²´ í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
  else {
    const allWords = whisperResponse.text.trim().split(/\s+/).filter(w => w.length > 0);
    const wordDuration = whisperResponse.duration / allWords.length;
    
    allWords.forEach((word, index) => {
      words.push({
        word,
        startTime: timeOffset + index * wordDuration,
        endTime: timeOffset + (index + 1) * wordDuration,
        confidence: 1,
      });
    });
  }

  return {
    fullText: whisperResponse.text,
    words,
    duration: whisperResponse.duration,
    language: whisperResponse.language,
  };
}

/**
 * ì—¬ëŸ¬ ì²­í¬ì˜ STT ê²°ê³¼ ë³‘í•©
 */
export function mergeSTTResults(results: STTResult[]): STTResult {
  if (results.length === 0) {
    return { fullText: '', words: [], duration: 0 };
  }
  
  if (results.length === 1) {
    return results[0];
  }
  
  const merged: STTResult = {
    fullText: results.map(r => r.fullText).join(' '),
    words: results.flatMap(r => r.words),
    duration: results.reduce((sum, r) => sum + r.duration, 0),
    language: results[0].language,
  };
  
  return merged;
}

// ============================================
// í†µí•© í•¨ìˆ˜ (ëŒ€ìš©ëŸ‰ ì§€ì›)
// ============================================

/**
 * ë¹„ë””ì˜¤ íŒŒì¼ì—ì„œ ìŒì„± ì¸ì‹ ìˆ˜í–‰ (ì „ì²´ íŒŒì´í”„ë¼ì¸, ëŒ€ìš©ëŸ‰ ì§€ì›)
 */
export async function transcribeVideo(
  videoFile: File,
  apiKey: string,
  onProgress?: (status: string) => void
): Promise<STTResult> {
  try {
    // 1. íŒŒì¼ í¬ê¸° ì²´í¬
    const sizeCheck = checkFileSize(videoFile);
    if (!sizeCheck.valid) {
      throw new Error(sizeCheck.message);
    }
    
    // 2. ì˜¤ë””ì˜¤ ì¶”ì¶œ
    onProgress?.('ğŸµ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘...');
    let audioBlob: Blob;
    
    try {
      audioBlob = await extractAudioFromVideo(videoFile, onProgress);
    } catch {
      // ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë³¸ íŒŒì¼ ì‚¬ìš©
      audioBlob = videoFile;
    }
    
    // 3. ì²­í¬ ë¶„í•  (ëŒ€ìš©ëŸ‰ íŒŒì¼)
    onProgress?.('ğŸ“¦ íŒŒì¼ ì²˜ë¦¬ ì¤‘...');
    const chunks = await splitAudioIntoChunks(audioBlob, CHUNK_DURATION_SECONDS, onProgress);
    
    // 4. ê° ì²­í¬ì— ëŒ€í•´ Whisper API í˜¸ì¶œ
    const results: STTResult[] = [];
    
    for (let i = 0; i < chunks.length; i++) {
      const chunk = chunks[i];
      onProgress?.(`ğŸ¤ ìŒì„± ì¸ì‹ ì¤‘... (${i + 1}/${chunks.length})`);
      
      const whisperResponse = await transcribeWithWhisper(chunk.blob, {
        apiKey,
        language: 'ko',
        responseFormat: 'verbose_json',
        timestampGranularities: ['word', 'segment'],
      });
      
      const result = convertWhisperToSTTResult(whisperResponse, chunk.startTime);
      results.push(result);
      
      // API ë ˆì´íŠ¸ ë¦¬ë°‹ ë°©ì§€
      if (i < chunks.length - 1) {
        await new Promise(resolve => setTimeout(resolve, 500));
      }
    }
    
    // 5. ê²°ê³¼ ë³‘í•©
    onProgress?.('ğŸ”— ê²°ê³¼ ë³‘í•© ì¤‘...');
    const mergedResult = mergeSTTResults(results);
    
    onProgress?.('âœ… ìŒì„± ì¸ì‹ ì™„ë£Œ!');
    return mergedResult;
    
  } catch (error) {
    console.error('STT ì˜¤ë¥˜:', error);
    throw error;
  }
}

// ============================================
// íŒŒì¼ í¬ê¸° ì²´í¬
// ============================================

/**
 * íŒŒì¼ í¬ê¸°ê°€ ì œí•œ ë‚´ì¸ì§€ í™•ì¸ (2GB)
 */
export function checkFileSize(file: File): { valid: boolean; message?: string } {
  if (file.size > MAX_FILE_SIZE) {
    return {
      valid: false,
      message: `íŒŒì¼ í¬ê¸°ê°€ ë„ˆë¬´ í½ë‹ˆë‹¤. (ìµœëŒ€ 2GB, í˜„ì¬ ${(file.size / (1024 * 1024 * 1024)).toFixed(2)}GB)`,
    };
  }
  return { valid: true };
}

/**
 * ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
 */
export function estimateProcessingTime(fileSize: number): string {
  const sizeMB = fileSize / (1024 * 1024);
  
  if (sizeMB < 25) {
    return 'ì•½ 30ì´ˆ ~ 1ë¶„';
  } else if (sizeMB < 100) {
    return 'ì•½ 1 ~ 3ë¶„';
  } else if (sizeMB < 500) {
    return 'ì•½ 3 ~ 10ë¶„';
  } else {
    return 'ì•½ 10 ~ 30ë¶„';
  }
}
